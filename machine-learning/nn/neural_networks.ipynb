{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Neural Networks #\n",
    "\n",
    "Neural networks (NN) are a set of machine learning techniques that are more generic than specific other machine algorithms (such as SVMs, k-means, etc). NNs can solve a wide class of problems using the same constructs, as opposed to non-NN algorithms which tend to use fairly different constructs compared to eachother.\n",
    "\n",
    "In addition, while in non-NN ML algorithms we often have to understand the problem spaces fairly well to apply the algorithms, NNs are much more generic and can solve a wide variety of problems, without us fully understanding how they solve those problems.\n",
    "\n",
    "Very good general overview: https://www.youtube.com/watch?v=aircAruvnKk\n",
    "\n",
    "Neural network example, with input, output and hidden layers:\n",
    "\n",
    "![Neutral Network](neural_net.png)\n",
    "\n",
    "Individual **neurons** are just sum or **\"transfer\"** functions:\n",
    "\n",
    "![Sum Function](sum-function.png)\n",
    "\n",
    "In most cases they have activation functions that will output constant or variable values if the output of the neuron reaches a certain threshold:\n",
    "\n",
    "![Activation Function](activation-function.png)\n",
    "\n",
    "Note that $w_{ij}$ represents the weights for input $i$ in neuron $j$ which can be represented as a matrix.\n",
    "\n",
    "\n",
    "There are multiple types of activation functions. Some popular ones:\n",
    " - [Sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function), ```σ(x)σ(x)```: squashes numbers into the range (0, 1)\n",
    " - [Softmax](https://en.wikipedia.org/wiki/Softmax_function): Generalization of the sigmoid function of k-dimensional vectors, returning a vector of dimension k with real values in range of (0,1) **that add up to 1** (=normalization, this is different from sigmoid, where this sum doesn't need to add up to 1). Can for example be used to represent the probability that a given input belongs to one of k output classes.\n",
    " - The [rectified linear unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)), ```ReLU(x)=max(0,x)```. This is quickly becoming one of the more popular activation functions, especially in Deep Learning, as it turns out that it's performance is better than the sigmoid function (and the function is simpler).\n",
    " - The [hyperbolic tangent](https://en.wikipedia.org/wiki/Hyperbolic_function#Standard_analytic_expressions): ```tanh(x)```, which squashes numbers into the range (-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types ####\n",
    "There are different neural network types:\n",
    "\n",
    "![Neural Network Types](neural-network-types.jpg)\n",
    "    \n",
    "    \n",
    "**Perceptron**: no hidden layers, only input and output.\n",
    "\n",
    "**Feed Forward**: No cycles or loops in the network.\n",
    "\n",
    "**Deep Neural Networks**: neural networks that contain more than one hidden layer. \n",
    "\n",
    "**Recurrent Neural Network (RNN)**: also propagate data from later processing stages to earlier stages.\n",
    "\n",
    "**Markov Chain**: You can go from any state to any other state. Probability to go from one state to another represented by a matrix.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks ##\n",
    "\n",
    "A RNN maintains internal memories about the world (weights assigned to different pieces of information) to help perform its classifications. For example, when classifying activities in movie clips, it will \"remember\" what has happened in previous clips.\n",
    "\n",
    "![Recurrent Neural Network](rnn.png)\n",
    "\n",
    "\n",
    "In this image, $\\phi$ is the activation function, $W$ is the weights matrix associated with the current state, $U$ is the weights matrix associated with the previous state.\n",
    "\n",
    "This can then in programming terms be interpreted as running a fixed program with certain inputs and some internal variables. \n",
    "\n",
    "A very simple implemtentation of an RNN might look like this (from http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "  # ...\n",
    "  def step(self, x):\n",
    "    # update the hidden state\n",
    "    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n",
    "    # compute the output vector\n",
    "    y = np.dot(self.W_hy, self.h)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how this compares to the picture above. In particular, note the following similarilites:\n",
    "- the ```np.tanh``` activation function, which squashes the output between -1 and 1\n",
    "- $h_t$ from the previous is called ```self.h```\n",
    "- The terms in the sum inbetween ```np.tanh(...)``` are switched here, it's basically: ```np.tanh(prev state + current state)```, while the image above does $\\phi (current + prev)$\n",
    "- $W$ is called ```self.W_xh```, $U$ is called ```self.W_hh```\n",
    "- in math terms, the code really does: $h_t = \\tanh ( W_{hh} h_{t-1} + W_{xh} x_t ))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long short-term memory Networks (LTSM) ##\n",
    "\n",
    "A lot of the info that follows is based off this blogpost: http://blog.echen.me/2017/05/30/exploring-lstms/\n",
    "\n",
    "Whereas an RNN can overwrite its memory at each time step in a fairly uncontrolled fashion, an LSTM (specific type of RNN) transforms its memory in a very precise way: by using specific learning mechanisms for which pieces of information to remember, which to update, and which to pay attention to. This helps it keep track of information over longer periods of time.\n",
    "\n",
    "In the code above, and LTSM would make the computation of ```self.h``` more complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNN) #\n",
    "\n",
    "Mostly used for visual recognition tasks (e.g. feature detection in images). Inspired by the human vision-related neural system. In CNNs, not every neuron in a layer is connected to every other neuron in the next layer. Instead neurons are only connected to *some* neurons in the next layer. In a sense, layers get aggregated into the next layer as shown in the next 2 images from [Hands-On Machine Learning with Scikit-Learn and TensorFlow](http://shop.oreilly.com/product/0636920052289.do).\n",
    "\n",
    "![CNN](cnn.png)\n",
    "\n",
    "More detailed:\n",
    "\n",
    "![CNN](cnn-detail.png)\n",
    "\n",
    "\n",
    "Note that this is different to connecting every pixel/neutron to every other pixel/neutron in the next layer (this is how traditional RNNs work). This is also how the human vision neural system works: neurons only have a limited **receptive field** (i.e. The neurons that can trigger a particular neuron are in the local neighborhood and limited to a small area). Neurons have a form of hierarchy in which some neurons that detect lower level structure (e.g.: lines) trigger neurons that recognize higher-level structure (e.g. shapes), and so on.\n",
    "\n",
    "\n",
    "In a CNN, A neuron's weights can be represented as a small image the size of the receptive field. These sets of weights are also called **filters** or **convolution kernels**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "(Joris: Pooling layers seem to be specific to CNNs.)\n",
    "\n",
    "Pooling layers group/pool inputs together into a smaller layer. This reduces the computational cost by reducing the number of parameters to learn, while often still providing good results. For example, often times, not all input numbers are needed to detect a feature.\n",
    "\n",
    "For example, in max pooling, we just take the highest (=max) number of each window we're considering. In the example below, the filter size = 2 (because the window=2x2), and the stride = 2, because we jump to steps to the right/bottom after applying the filter (i.e. there is no overlap between consecutive filter applications, which would be the case if stride was e.g. equal to 1).\n",
    "\n",
    "![Max Pooling](max-pooling.webp)\n",
    "\n",
    "Average Pooling is sometimes also used (averaging the numbers in the window), but this isn't often used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Auto-encoders\n",
    "\n",
    "From \"Deep Learning using Tensorflow\" \n",
    "\n",
    "![Auto-encoder](auto-encoder.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intent of this notebook is to document different types of machine learning approaches and algorithms. At this point it's a pile of everything, I expect to break this up into more modular pieces as more and more content is added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning\n",
    "Wikipedia has an excellent definition:\n",
    "\n",
    "**Supervised learning** is the machine learning task of inferring a function from labeled training data.\n",
    "The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification ##\n",
    "### Support Vector Machines (SVMs) ###\n",
    "SVMs are a set of learning algorithms for to do classification. In a nutshell, SVMs try to find a hyperplane that maximes the distance between clusters of data.\n",
    "\n",
    "Visually:\n",
    "\n",
    "![SVM Example](svm/svm-example.png)\n",
    "\n",
    "The points/vectors that define where the hyperplane fits are called the **support vectors** (they're \"supporting\" the maximum distance between the clusters and the plane).\n",
    "\n",
    "\n",
    "![Support Vectors](svm/support-vectors.png)\n",
    "\n",
    "\n",
    "Computing SVMs comes down to solving a differential equation: finding a hyperplane that minimizes the distance between the hyperplane and all potential support vectors in their respective clusters. There are multiple algorithms for solving this.\n",
    "\n",
    "![Support Vectors](svm/svm-minimize.png)\n",
    "\n",
    "Note that a tricky part of defining an SVMs is often finding the right feature space. What might be hard in one dimension, might become easy in a higher dimension that represents the right features:\n",
    "\n",
    "![Support Vectors](svm/input-feature-space.png)\n",
    "\n",
    "\n",
    "#### SVMs in  Scikit-learn ####\n",
    "\n",
    "In Scikit-learn, the simplest SVM that corresponds to the pictures above is ```LinearSVC```. The algorithm takes a ```C``` value which represents the amount of error/margin the SVM will allow when making calculations. The images below visualize the difference between C values (from [Hands-On Machine Learning with Scikit-Learn and TensorFlow](http://shop.oreilly.com/product/0636920052289.do))\n",
    "\n",
    "\n",
    "![C value in scikit-learn](svm/svm-scikit-learn-C.png)\n",
    "\n",
    "The question is which ```C``` value better captures the true nature relationship of the data. A ```C``` value that is too high might cause overfitting: the model might be too much tailored to the learning data and not generalize well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Neural Networks #\n",
    "\n",
    "Neural network example, with input, output and hidden layers:\n",
    "\n",
    "![Neutral Network](nn/neural_net.png)\n",
    "\n",
    "Individual **neurons** are just sum or **\"transfer\"** functions:\n",
    "\n",
    "![Sum Function](nn/sum-function.png)\n",
    "\n",
    "In most cases they have activation functions that will output constant or variable values if the output of the neuron reaches a certain threshold:\n",
    "\n",
    "![Activation Function](nn/activation-function.png)\n",
    "\n",
    "Note that $w_{ij}$ represents the weights for input $i$ in neuron $j$ which can be represented as a matrix.\n",
    "\n",
    "\n",
    "There are multiple types of activation functions. Some popular ones:\n",
    " - [Sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function), ```σ(x)σ(x)```: squashes numbers into the range (0, 1)\n",
    " - The [hyperbolic tangent](https://en.wikipedia.org/wiki/Hyperbolic_function#Standard_analytic_expressions): ```tanh(x)```, which squashes numbers into the range (-1, 1)\n",
    " - The [rectified linear unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)), ```ReLU(x)=max(0,x)```.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Types ####\n",
    "There are different neural network types:\n",
    "\n",
    "![Neural Network Types](nn/neural-network-types.jpg)\n",
    "    \n",
    "    \n",
    "**Perceptron**: no hidden layers, only input and output.\n",
    "\n",
    "**Feed Forward**: No cycles or loops in the network.\n",
    "\n",
    "**Deep Neural Networks**: neural networks that contain more than one hidden layer. \n",
    "\n",
    "**Recurrent Neural Network (RNN)**: also propagate data from later processing stages to earlier stages.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Recurrent Neural Networks ##\n",
    "\n",
    "A RNN maintains internal memories about the world (weights assigned to different pieces of information) to help perform its classifications. For example, when classifying activities in movie clips, it will \"remember\" what has happened in previous clips.\n",
    "\n",
    "![Recurrent Neural Network](nn/rnn.png)\n",
    "\n",
    "\n",
    "In this image, $\\phi$ is the activation function, $W$ is the weights matrix associated with the current state, $U$ is the weights matrix associated with the previous state.\n",
    "\n",
    "This can then in programming terms be interpreted as running a fixed program with certain inputs and some internal variables. \n",
    "\n",
    "A very simple implemtentation of an RNN might look like this (from http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "  # ...\n",
    "  def step(self, x):\n",
    "    # update the hidden state\n",
    "    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n",
    "    # compute the output vector\n",
    "    y = np.dot(self.W_hy, self.h)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how this compares to the picture above. In particular, note the following similarilites:\n",
    "- the ```np.tanh``` activation function, which squashes the output between -1 and 1\n",
    "- $h_t$ from the previous is called ```self.h```\n",
    "- The terms in the sum inbetween ```np.tanh(...)``` are switched here, it's basically: ```np.tanh(prev state + current state)```, while the image above does $\\phi (current + prev)$\n",
    "- $W$ is called ```self.W_xh```, $U$ is called ```self.W_hh```\n",
    "- in math terms, the code really does: $h_t = \\tanh ( W_{hh} h_{t-1} + W_{xh} x_t ))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Long short-term memory Networks (LTSM) ##\n",
    "\n",
    "A lot of the info that follows is based off this blogpost: http://blog.echen.me/2017/05/30/exploring-lstms/\n",
    "\n",
    "Whereas an RNN can overwrite its memory at each time step in a fairly uncontrolled fashion, an LSTM (specific type of RNN) transforms its memory in a very precise way: by using specific learning mechanisms for which pieces of information to remember, which to update, and which to pay attention to. This helps it keep track of information over longer periods of time.\n",
    "\n",
    "In the code above, and LTSM would make the computation of ```self.h``` more complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNN) #\n",
    "\n",
    "Mostly used for visual recognition tasks (e.g. feature detection in images). Inspired by the human vision-related neural system. In CNNs, not every neuron in a layer is connected to every other neuron in the next layer. Instead neurons are only connected to *some* neurons in the next layer. In a sense, layers get aggregated into the next layer as shown in the next 2 images from [Hands-On Machine Learning with Scikit-Learn and TensorFlow](http://shop.oreilly.com/product/0636920052289.do).\n",
    "\n",
    "![CNN](nn/cnn.png)\n",
    "\n",
    "More detailed:\n",
    "\n",
    "![CNN](nn/cnn-detail.png)\n",
    "\n",
    "\n",
    "Note that this is different to connecting every pixel/neutron to every other pixel/neutron in the next layer (this is how traditional RNNs work). This is also how the human vision neural system works: neurons only have a limited **receptive field** (i.e. The neurons that can trigger a particular neuron are in the local neighborhood and limited to a small area). Neurons have a form of hierarchy in which some neurons that detect lower level structure (e.g.: lines) trigger neurons that recognize higher-level structure (e.g. shapes), and so on.\n",
    "\n",
    "\n",
    "In a CNN, A neuron's weights can be represented as a small image the size of the receptive field. These sets of weights are also called **filters** or **convolution kernels**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "With unsupervised learning, there are no output values or labels provided, the algorithms find those themselves.\n",
    "A good example of unsupervised learning is finding clusters in a dataset: you don't need to specify how many clusters there, or what they are named, etc. An unsupervised clustering algorithm will find the clusters in the data without further input. A practical example is grouping photos of the same person on e.g. Google Photos or Facebook.\n",
    "\n",
    "Example learning algorithms:\n",
    "- Clustering:\n",
    "    - k-means\n",
    "    - Hierarchical Cluster Analysis (HCA)\n",
    "    - Expected Maximization\n",
    "- Association rule learning:\n",
    "    - Aprioiri\n",
    "    - Eclat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means\n",
    "\n",
    "Most of this is from [wikipedia](https://en.wikipedia.org/wiki/K-means_clustering).\n",
    "\n",
    "k-means clustering aims to partition n observations into k clusters.\n",
    "\n",
    "Given a set of observations $(x_1, x_2, …, x_n)$, where each observation is a $d$-dimensional real vector, k-means clustering aims to partition the $n$ observations into $k (≤ n)$ sets $S = {S_1, S_2, …, S_k}$ so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). Formally, the objective is to find:\n",
    "\n",
    "$${\\displaystyle {\\underset {\\mathbf {S} }{\\operatorname {arg\\,min} }}\\sum _{i=1}^{k}\\sum _{\\mathbf {x} \\in S_{i}}\\left\\|\\mathbf {x} -{\\boldsymbol {\\mu }}_{i}\\right\\|^{2}={\\underset {\\mathbf {S} }{\\operatorname {arg\\,min} }}\\sum _{i=1}^{k}|S_{i}|\\operatorname {Var} S_{i}}$$\n",
    "\n",
    "where $μ_i$ is the mean of points in $S_i$. \n",
    "\n",
    "### algorithms\n",
    "The problem is computationally difficult (NP-hard); however, there are efficient heuristic algorithms that are commonly employed and converge quickly to a local optimum. So these algorithms don't typically find the best (=optimal) global clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "In reinforement learning, an _agent_ observes the enforment and is able to select and perform action and it get's _rewards_ in return (or _penalties_). It must then learn by itself what the best strategy or policy is.\n",
    "\n",
    "This approach has had recent successes with e.g. Deepmind's AlphaGo program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch and Online Learning\n",
    "\n",
    "In batch learning a system cannot learn incrementally, it needs to have all the data available when it does learning. The system is first trained and then released in production. Typically, batch systems are retrained e.g.: every hour, 24hours, week, etc. This is called **offline learning**.\n",
    "\n",
    "In online learning, the learning happens simultaneously with outputting values, this is often useful when there is a cotinuous flow of data (e.g. stock prices). An important principle here is that for the *learning rate*: how fast does the system adapt to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "Building a model on top of many other models is called *Ensemble Learning*, and it is often a great way to push ML algorithms even further.\n",
    "\n",
    "An example is a ```RandomForestRegressor``` (scikit-learn). Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "The intent of this notebook is to document different types of machine learning approaches and algorithms. At this point it's a pile of everything, I expect to break this up into more modular pieces as more and more content is added.\n",
    "\n",
    "Neural networks are already broken out into [nn/neural_networks.ipynb](nn/neural_networks.ipynb)\n",
    "\n",
    "ML vs. Tranditional Programming:\n",
    "![ML vs. Traditional Programming](images/ml-vs-traditional-programming.png)\n",
    "\n",
    "The rules are often expressed using weights/numbers (like e.g. Weight matrix in NNs).\n",
    "\n",
    "Source: https://www.bouvet.no/bouvet-deler/6-tips-for-getting-started-with-machine-learning\n",
    "\n",
    "\n",
    "The 2 image below gives a good idea of the different subfields of ML:\n",
    "\n",
    "![AI vs. ML vs. Deep Learning](images/ai_ml_deeplearning.jpg)\n",
    "\n",
    "Source: https://www.mathworks.com/discovery/machine-learning.html\n",
    "\n",
    "![Machine Learning hierarchy](images/machine_learning.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning\n",
    "Wikipedia has an excellent definition:\n",
    "\n",
    "**Supervised learning** is the machine learning task of inferring a function from labeled training data.\n",
    "The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification ##\n",
    "### Support Vector Machines (SVMs) ###\n",
    "SVMs are a set of learning algorithms for to do classification. In a nutshell, SVMs try to find a hyperplane that maximes the distance between clusters of data.\n",
    "\n",
    "Visually:\n",
    "\n",
    "![SVM Example](svm/svm-example.png)\n",
    "\n",
    "The points/vectors that define where the hyperplane fits are called the **support vectors** (they're \"supporting\" the maximum distance between the clusters and the plane).\n",
    "\n",
    "\n",
    "![Support Vectors](svm/support-vectors.png)\n",
    "\n",
    "\n",
    "Computing SVMs comes down to solving a differential equation: finding a hyperplane that minimizes the distance between the hyperplane and all potential support vectors in their respective clusters. There are multiple algorithms for solving this.\n",
    "\n",
    "![Support Vectors](svm/svm-minimize.png)\n",
    "\n",
    "Note that a tricky part of defining an SVMs is often finding the right feature space. What might be hard in one dimension, might become easy in a higher dimension that represents the right features:\n",
    "\n",
    "![Support Vectors](svm/input-feature-space.png)\n",
    "\n",
    "\n",
    "#### SVMs in  Scikit-learn ####\n",
    "\n",
    "In Scikit-learn, the simplest SVM that corresponds to the pictures above is ```LinearSVC```. The algorithm takes a ```C``` value which represents the amount of error/margin the SVM will allow when making calculations. The images below visualize the difference between C values (from [Hands-On Machine Learning with Scikit-Learn and TensorFlow](http://shop.oreilly.com/product/0636920052289.do))\n",
    "\n",
    "\n",
    "![C value in scikit-learn](svm/svm-scikit-learn-C.png)\n",
    "\n",
    "The question is which ```C``` value better captures the true nature relationship of the data. A ```C``` value that is too high might cause overfitting: the model might be too much tailored to the learning data and not generalize well.\n",
    "\n",
    "\n",
    "\n",
    "#### Detecting multiple classes using SVMs ####\n",
    "\n",
    "SVMs are binary classifiers: they can inheritly only classify 2 classes (as the found function divides the hyperplane in 2). Still, SVMs are often used to find multiple classes by training and SVM for each of the target classes and then comparing fit or loss scores. For example, when solving the classic MNIST digit classification problem using an SVM, as done in [sk-learn-intro.ipynb](../sk-learn-intro.ipynb), we train 10 SVMs, one for each digit. The SVM for digit 5 then can recognize whether a digit is either 5 or not 5, with a certain associated accuracy. When classifing a random digit, we simply run all SVMs and compare their scores and pick the one with the highest score.\n",
    "\n",
    "Note that scikit-learn takes care of running all 10 SVMs and selecting the class with the highest score automatically for us.\n",
    "\n",
    "This strategy of training *x* SVMs for *x* target classes, where each SVM can only distinguish between its class and *everything else* is called the *one-versus-all (OvA)* strategy. This is the default in scikit-learn. An alternative strategy, *one-versus-one (OvO)* - ```OneVsOneClassifier```, creates SVMs that can detect pairs of classes (e.g. SVM that can detect 5 or 6). A downside of this approach is that you need to create SVMs for all possible pairs. For MNIST this means a combination of 2 out of 10:\n",
    "\n",
    "$$C(10,2) =\\frac{10!}{(2!(10−2)!)} = 45$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees #\n",
    "\n",
    "Scikit-Learn uses the CART algorithm, which produces only binary trees: nonleaf nodes always have two children (i.e., questions only have yes/no answers). There are other algorithms that use produce 3 or more leafs per parent.\n",
    "\n",
    "From [Hands-On Machine Learning with Scikit-Learn and TensorFlow, Chapter 6](http://shop.oreilly.com/product/0636920052289.do):\n",
    "The idea behind the CART algorithm is quite simple: the algorithm first splits the training set in two subsets using a single feature k and a threshold tk (e.g., \"petal length <= 2.45 cm\"). How does it choose $k$ and $t_k$? It searches for the pair $(k, tk)$ that produces the purest subsets (weighted by their size). Purity is defined by the [Gini measure](https://en.wikipedia.org/wiki/Gini_coefficient). Once it has successfully split the training set in two, it splits the subsets using the same logic, then the sub-subsets and so on, recursively. It stops recursing once it reaches the maximum depth (defined by the max_depth hyperparameter), or if it cannot find a split that will reduce impurity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "With unsupervised learning, there are no output values or labels provided, the algorithms find those themselves.\n",
    "A good example of unsupervised learning is finding clusters in a dataset: you don't need to specify how many clusters there, or what they are named, etc. An unsupervised clustering algorithm will find the clusters in the data without further input. A practical example is grouping photos of the same person on e.g. Google Photos or Facebook.\n",
    "\n",
    "Example learning algorithms:\n",
    "- Clustering:\n",
    "    - k-means\n",
    "    - Hierarchical Cluster Analysis (HCA)\n",
    "    - Expected Maximization\n",
    "- Association rule learning:\n",
    "    - Aprioiri\n",
    "    - Eclat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means\n",
    "\n",
    "Most of this is from [wikipedia](https://en.wikipedia.org/wiki/K-means_clustering).\n",
    "\n",
    "k-means clustering aims to partition n observations into k clusters.\n",
    "\n",
    "Given a set of observations $(x_1, x_2, …, x_n)$, where each observation is a $d$-dimensional real vector, k-means clustering aims to partition the $n$ observations into $k (≤ n)$ sets $S = {S_1, S_2, …, S_k}$ so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). Formally, the objective is to find:\n",
    "\n",
    "$${\\displaystyle {\\underset {\\mathbf {S} }{\\operatorname {arg\\,min} }}\\sum _{i=1}^{k}\\sum _{\\mathbf {x} \\in S_{i}}\\left\\|\\mathbf {x} -{\\boldsymbol {\\mu }}_{i}\\right\\|^{2}={\\underset {\\mathbf {S} }{\\operatorname {arg\\,min} }}\\sum _{i=1}^{k}|S_{i}|\\operatorname {Var} S_{i}}$$\n",
    "\n",
    "where $μ_i$ is the mean of points in $S_i$. \n",
    "\n",
    "### algorithms\n",
    "The problem is computationally difficult (NP-hard); however, there are efficient heuristic algorithms that are commonly employed and converge quickly to a local optimum. So these algorithms don't typically find the best (=optimal) global clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "In reinforement learning, an _agent_ observes the enforment and is able to select and perform action and it get's _rewards_ in return (or _penalties_). It must then learn by itself what the best strategy or policy is.\n",
    "\n",
    "This approach has had recent successes with e.g. Deepmind's AlphaGo program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch and Online Learning\n",
    "\n",
    "In batch learning a system cannot learn incrementally, it needs to have all the data available when it does learning. The system is first trained and then released in production. Typically, batch systems are retrained e.g.: every hour, 24hours, week, etc. This is called **offline learning**.\n",
    "\n",
    "In online learning, the learning happens simultaneously with outputting values, this is often useful when there is a cotinuous flow of data (e.g. stock prices). An important principle here is that for the *learning rate*: how fast does the system adapt to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "Building a model on top of many other models is called *Ensemble Learning*, and it is often a great way to push ML algorithms even further.\n",
    "\n",
    "An example is a ```RandomForestRegressor``` (scikit-learn). Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Transfer learning focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.\n",
    "\n",
    "Practically, you can for example take a model from [TensorFlow Hub](https://www.tensorflow.org/hub/) and then apply it to your own problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

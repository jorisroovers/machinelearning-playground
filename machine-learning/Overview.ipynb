{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intent of this notebook is to document different types of machine learning approaches and algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning\n",
    "Wikipedia has an excellent definition:\n",
    "\n",
    "**Supervised learning** is the machine learning task of inferring a function from labeled training data.\n",
    "The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification ##\n",
    "### Support Vector Machines (SVMs) ###\n",
    "SVMs are a set of learning algorithms for to do classification. In a nutshell, SVMs try to find a hyperplane that maximes the distance between clusters of data.\n",
    "\n",
    "Visually:\n",
    "\n",
    "![SVM Example](svm/svm-example.png)\n",
    "\n",
    "The points/vectors that define where the hyperplane fits are called the **support vectors** (they're \"supporting\" the maximum distance between the clusters and the plane).\n",
    "\n",
    "\n",
    "![Support Vectors](svm/support-vectors.png)\n",
    "\n",
    "\n",
    "Computing SVMs comes down to solving a differential equation: finding a hyperplane that minimizes the distance between the hyperplane and all potential support vectors in their respective clusters. There are multiple algorithms for solving this.\n",
    "\n",
    "![Support Vectors](svm/svm-minimize.png)\n",
    "\n",
    "Note that a tricky part of defining an SVMs is often finding the right feature space. What might be hard in one dimension, might become easy in a higher dimension that represents the right features:\n",
    "\n",
    "![Support Vectors](svm/input-feature-space.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks ###\n",
    "\n",
    "Neural network example, with input, output and hidden layers:\n",
    "\n",
    "![Neutral Network](nn/neural_net.png)\n",
    "\n",
    "Individual **neurons** are just sum or **\"transfer\"** functions:\n",
    "\n",
    "![Sum Function](nn/sum-function.png)\n",
    "\n",
    "In most cases they have activation functions that will output constant or variable values if the output of the neuron reaches a certain threshold:\n",
    "\n",
    "![Activation Function](nn/activation-function.png)\n",
    "\n",
    "Note that $w_{ij}$ represents the weights for input $i$ in neuron $j$ which can be represented as a matrix.\n",
    "\n",
    "\n",
    "There are multiple types of activation functions. Some popular ones:\n",
    " - [Sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function), ```σ(x)σ(x)```: squashes numbers into the range (0, 1)\n",
    " - The [hyperbolic tangent](https://en.wikipedia.org/wiki/Hyperbolic_function#Standard_analytic_expressions): ```tanh(x)```, which squashes numbers into the range (-1, 1)\n",
    " - The [rectified linear unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)), ```ReLU(x)=max(0,x)```.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Types ####\n",
    "There are different neural network types:\n",
    "\n",
    "![Neural Network Types](nn/neural-network-types.jpg)\n",
    "    \n",
    "    \n",
    "**Perceptron**: no hidden layers, only input and output.\n",
    "\n",
    "**Feed Forward**: No cycles or loops in the network.\n",
    "\n",
    "**Deep Neural Networks**: neural networks that contain more than one hidden layer. \n",
    "\n",
    "**Recurrent Neural Network (RNN)**: also propagate data from later processing stages to earlier stages.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Recurrent Neural Networks##\n",
    "\n",
    "A RNN maintains internal memories about the world (weights assigned to different pieces of information) to help perform its classifications. For example, when classifying activities in movie clips, it will \"remember\" what has happened in previous clips.\n",
    "\n",
    "![Recurrent Neural Network](nn/rnn.png)\n",
    "\n",
    "\n",
    "In this image, $\\phi$ is the activation function, $W$ is the weights matrix associated with the current state, $U$ is the weights matrix associated with the previous state.\n",
    "\n",
    "This can then in programming terms be interpreted as running a fixed program with certain inputs and some internal variables. \n",
    "\n",
    "A very simple implemtentation of an RNN might look like this (from http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "  # ...\n",
    "  def step(self, x):\n",
    "    # update the hidden state\n",
    "    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n",
    "    # compute the output vector\n",
    "    y = np.dot(self.W_hy, self.h)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how this compares to the picture above. In particular, note the following similarilites:\n",
    "- the ```np.tanh``` activation function, which squashes the output between -1 and 1\n",
    "- $h_t$ from the previous is called ```self.h```\n",
    "- The terms in the sum inbetween ```np.tanh(...)``` are switched here, it's basically: ```np.tanh(prev state + current state)```, while the image above does $\\phi (current + prev)$\n",
    "- $W$ is called ```self.W_xh```, $U$ is called ```self.W_hh```\n",
    "- in math terms, the code really does: $h_t = \\tanh ( W_{hh} h_{t-1} + W_{xh} x_t ))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Long short-term memory Networks (LTSM) ##\n",
    "\n",
    "A lot of the info that follows is based off this blogpost: http://blog.echen.me/2017/05/30/exploring-lstms/\n",
    "\n",
    "Whereas an RNN can overwrite its memory at each time step in a fairly uncontrolled fashion, an LSTM (specific type of RNN) transforms its memory in a very precise way: by using specific learning mechanisms for which pieces of information to remember, which to update, and which to pay attention to. This helps it keep track of information over longer periods of time.\n",
    "\n",
    "In the code above, and LTSM would make the computation of ```self.h``` more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
